{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow multiple displays per cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import shutil\n",
    "import urllib\n",
    "import azureml.core\n",
    "from azureml.core import Workspace, Experiment\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core.datastore import Datastore\n",
    "import platform,  dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure ML SDK Version:  1.0.74\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Linux-4.15.0-1063-azure-x86_64-with-debian-10.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'/workspace/code'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Azure ML SDK Version: \", azureml.core.VERSION)\n",
    "platform.platform()\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nihchestxraymetadata_ds_name='nihchestxraymetadata_ds'\n",
    "nihchestxrayimages_ds_name='nihchestxrayimages_ds'\n",
    "experiment_name = \"chestxray-keras\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credentials file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./../not_shared/general.env'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dotenv_file_path = './../not_shared/' \n",
    "os.makedirs(dotenv_file_path, exist_ok=True)\n",
    "dotenv_file_path = os.path.join(dotenv_file_path, 'general.env')\n",
    "dotenv_file_path\n",
    "! sudo touch $dotenv_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscription_id=\"\"\n",
    "resource_group=\"\"\n",
    "rsg_region=\"eastus\"\n",
    "\n",
    "workspace_name = 'ghiordanazurechstxrayws'\n",
    "workspace_region = rsg_region\n",
    "\n",
    "sa_name=\"\"\n",
    "sa_key=\"\"\n",
    "sa_container_name=\"nihchestxraydata\"\n",
    "sa_container_name_compressed_data=\"nihchestxraydatacompressed\"\n",
    "sa_container_name_images=\"nihchestxrayimages\"\n",
    "\n",
    "gpu_cluster_name = 'azuremlchestxray'\n",
    "gpucluster_admin_user_name=\"\"\n",
    "gpucluster_admin_user_password = \"\"\n",
    "\n",
    "docker_login=\"georgedockeraccount\"\n",
    "docker_password=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = dotenv.set_key(dotenv_file_path, 'subscription_id', subscription_id)\n",
    "# response = dotenv.set_key(dotenv_file_path, 'resource_group', resource_group)\n",
    "# response = dotenv.set_key(dotenv_file_path, 'rsg_region', rsg_region)\n",
    "\n",
    "# response = dotenv.set_key(dotenv_file_path, 'workspace_name', workspace_name)\n",
    "# response = dotenv.set_key(dotenv_file_path, 'workspace_region', workspace_region)\n",
    "\n",
    "# response = dotenv.set_key(dotenv_file_path, 'sa_name', sa_name)\n",
    "# response = dotenv.set_key(dotenv_file_path, 'sa_key', sa_key)\n",
    "# response = dotenv.set_key(dotenv_file_path, 'sa_container_name', sa_container_name)\n",
    "# response = dotenv.set_key(dotenv_file_path, 'sa_container_name_compressed_data', sa_container_name_compressed_data)\n",
    "# response = dotenv.set_key(dotenv_file_path, 'sa_container_name_images', sa_container_name_images)\n",
    "\n",
    "# response = dotenv.set_key(dotenv_file_path, 'gpu_cluster_name', gpu_cluster_name)\n",
    "# response = dotenv.set_key(dotenv_file_path, 'gpucluster_admin_user_name', gpucluster_admin_user_name)\n",
    "# response = dotenv.set_key(dotenv_file_path, 'gpucluster_admin_user_password', gpucluster_admin_user_password)\n",
    "\n",
    "# response = dotenv.set_key(dotenv_file_path, 'docker_login', docker_login)\n",
    "# response = dotenv.set_key(dotenv_file_path, 'docker_password', docker_password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv $dotenv_file_path -o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cat $dotenv_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscription_id = os.getenv('subscription_id')\n",
    "# # print a bit of subscription ID, to show dotenv file was found and loaded \n",
    "# subscription_id[:2]\n",
    "\n",
    "crt_resource_group  = os.getenv('resource_group')\n",
    "crt_workspace_name = os.getenv('workspace_name')\n",
    "crt_workspace_region = os.getenv('workspace_region')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace configuration loading succeeded. \n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    ws1 = Workspace(\n",
    "        subscription_id = subscription_id, \n",
    "        resource_group = crt_resource_group, \n",
    "        workspace_name = crt_workspace_name)\n",
    "    print(\"Workspace configuration loading succeeded. \")\n",
    "#     ws1.write_config()\n",
    "    del ws1 # ws will be (re)created later using from_config() function\n",
    "except Exception as e :\n",
    "    print('Exception msg: {}'.format(str(e )))\n",
    "    print(\"Workspace not accessible. Will create a new workspace below\")\n",
    "    \n",
    "    workspace_region = crt_workspace_region\n",
    "\n",
    "    # Create the workspace using the specified parameters\n",
    "    ws2 = Workspace.create(name = crt_workspace_name,\n",
    "                          subscription_id = subscription_id,\n",
    "                          resource_group = crt_resource_group, \n",
    "                          location = workspace_region,\n",
    "                          create_resource_group = False,\n",
    "                          exist_ok = False)\n",
    "    ws2.get_details()\n",
    "\n",
    "    # persist the subscription id, resource group name, and workspace name in aml_config/config.json.\n",
    "    ws2.write_config()\n",
    "    \n",
    "    #Delete ws2 and use ws = Workspace.from_config() as shwon below to recover the ws, rather than rely on what we get from one time creation\n",
    "    del ws2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#later accesses\n",
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make new Experiment in workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = Experiment(workspace=ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up a new datastore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create your own access keys file to store your secret credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = Datastore.register_azure_blob_container(workspace=ws, \n",
    "                                             datastore_name=nihchestxraymetadata_ds_name, \n",
    "                                             container_name=os.getenv('sa_container_name'),\n",
    "                                             account_name=os.getenv('sa_name'), \n",
    "                                             account_key=os.getenv('sa_key'),\n",
    "                                             create_if_not_exists=False,\n",
    "                                             overwrite=True)\n",
    "ds2 = Datastore.register_azure_blob_container(workspace=ws, \n",
    "                                             datastore_name=nihchestxrayimages_ds_name, \n",
    "                                             container_name=os.getenv('sa_container_name_images'),\n",
    "                                             account_name=os.getenv('sa_name'), \n",
    "                                             account_key=os.getenv('sa_key'),\n",
    "                                             create_if_not_exists=False,\n",
    "                                             overwrite=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workspaceblobstore AzureBlob\n",
      "workspacefilestore AzureFile\n",
      "nihchestxraymetadata_ds AzureBlob\n",
      "nihchestxrayimages_ds AzureBlob\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "$AZUREML_DATAREFERENCE_nihchestxraymetadata_ds"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "$AZUREML_DATAREFERENCE_nihchestxrayimages_ds"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#list all datastores registered in current workspace\n",
    "datastores = ws.datastores\n",
    "for name, ds in datastores.items():\n",
    "    print(name, ds.datastore_type)\n",
    "    \n",
    "Datastore.get(ws, nihchestxraymetadata_ds_name).as_mount()\n",
    "Datastore.get(ws, nihchestxrayimages_ds_name).as_mount()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the first run-through, upload images to datastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds.upload(src_dir = './../../../data', target_path = 'data', show_progress = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Compute Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing gpu cluster\n"
     ]
    }
   ],
   "source": [
    "# Choose a name for your GPU cluster\n",
    "gpu_cluster_name = \"gpucluster\"\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "max_nodes_value = 2\n",
    "min_nodes_value = 0\n",
    "try:\n",
    "    gpu_cluster = ComputeTarget(workspace=ws, name=gpu_cluster_name)\n",
    "    print(\"Found existing gpu cluster\")\n",
    "except ComputeTargetException:\n",
    "    print(\"Could not find ComputeTarget cluster!\")\n",
    "    \n",
    "#     Create a new gpucluster using code below\n",
    "    # Specify the configuration for the new cluster\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='Standard_NC24s_v2', # \"Standard_NC24s_v3\",\n",
    "                                                           min_nodes=0,\n",
    "                                                           max_nodes=max_nodes_value)\n",
    "    # Create the cluster with the specified name and configuration\n",
    "    gpu_cluster = ComputeTarget.create(ws, gpu_cluster_name, compute_config)\n",
    "\n",
    "    # Wait for the cluster to complete, show the output log\n",
    "    gpu_cluster.wait_for_completion(show_output=True)\n",
    "\n",
    "# for demo purposes, this is how you resize the cluster    \n",
    "gpu_cluster.update(min_nodes=min_nodes_value, max_nodes=max_nodes_value, idle_seconds_before_scaledown=1200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up script directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_folder = './xray_scripts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_data_filename = 'get_data.py'\n",
    "train_script_filename = 'train.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create script to copy and process data from Azure Blob Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-processing code organizes the images and labels, removes bad images, and splits the data into training, validation, and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {os.path.join(script_folder, get_data_filename)}\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "import src.azure_chestxray_utils\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import tqdm\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.model_selection \n",
    "\n",
    "\n",
    "def preprocess_data(base_dir):\n",
    "    #TODO -- get constants from separate file\n",
    "    prj_consts = src.azure_chestxray_utils.chestxray_consts()\n",
    "\n",
    "    data_base_input_dir=os.path.join(base_dir, os.path.join(*(prj_consts.BASE_INPUT_DIR_list)))\n",
    "    data_base_output_dir=os.path.join(base_dir, os.path.join(*(prj_consts.BASE_OUTPUT_DIR_list))) \n",
    "    nih_chest_xray_data_dir=os.path.join(data_base_input_dir, os.path.join(*(prj_consts.ChestXray_IMAGES_DIR_list)))\n",
    "    \n",
    "    data_partitions_dir=os.path.join(data_base_output_dir, os.path.join(*(prj_consts.DATA_PARTITIONS_DIR_list))) \n",
    "\n",
    "    partition_path = os.path.join(data_partitions_dir, 'partition14_unormalized_cleaned.pickle')\n",
    "    label_path = os.path.join(data_partitions_dir,'labels14_unormalized_cleaned.pickle')\n",
    "\n",
    "    # global variables\n",
    "\n",
    "    weights_dir = os.path.join(data_base_output_dir, os.path.join(*(prj_consts.MODEL_WEIGHTS_DIR_list))) \n",
    "    \n",
    "    #check number of images\n",
    "#     orig_images_no = subprocess.call(\"find $nih_chest_xray_data_dir -type f | wc -l\")\n",
    "#     print(\"orig images number:{} \".format(orig_images_no))\n",
    "    \n",
    "    #Train/Validation/Test Data partitioning\n",
    "    total_patient_number = 30805\n",
    "    NIH_annotated_file = 'BBox_List_2017.csv' # exclude from train pathology annotated by radiologists \n",
    "    manually_selected_bad_images_file = 'blacklist.csv'# exclude what viusally looks like bad images\n",
    "    \n",
    "    patient_id_original = [i for i in range(1,total_patient_number + 1)]\n",
    "    \n",
    "    other_data_dir=os.path.join(nih_chest_xray_data_dir, os.path.join(*(['..','..'])))\n",
    "    \n",
    "    # ignored images list is used later, since this is not a patient ID level issue\n",
    "    ignored_images_set = set()\n",
    "    with open(os.path.join(other_data_dir, manually_selected_bad_images_file), 'r') as f:\n",
    "        for line in f:\n",
    "            # delete the last char which is \\n\n",
    "            ignored_images_set.add(line[:-1])\n",
    "            if int(line[:-9]) >= 30805:\n",
    "                print(line[:-1])\n",
    "\n",
    "    bbox_df = pd.read_csv(os.path.join(other_data_dir, NIH_annotated_file))\n",
    "    bbox_patient_index_df = bbox_df['Image Index'].str.slice(3, 8)\n",
    "\n",
    "    bbox_patient_index_list = []\n",
    "    for index, item in bbox_patient_index_df.iteritems():\n",
    "        bbox_patient_index_list.append(int(item))\n",
    "\n",
    "    patient_id = list(set(patient_id_original) - set(bbox_patient_index_list))\n",
    "    print(\"len of original patient id is\", len(patient_id_original))\n",
    "    print(\"len of cleaned patient id is\", len(patient_id))\n",
    "    print(\"len of unique patient id with annotated data\", \n",
    "          len(list(set(bbox_patient_index_list))))\n",
    "    print(\"len of patient id with annotated data\",bbox_df.shape[0])\n",
    "    \n",
    "    random.seed(0)\n",
    "    random.shuffle(patient_id)\n",
    "\n",
    "    print(\"first ten patient ids are\", patient_id[:10])\n",
    "\n",
    "    # training:valid:test=7:1:2\n",
    "    patient_id_train = patient_id[:int(total_patient_number * 0.7)]\n",
    "    patient_id_valid = patient_id[int(total_patient_number * 0.7):int(total_patient_number * 0.8)]\n",
    "    # get the rest of the patient_id as the test set\n",
    "    patient_id_test = patient_id[int(total_patient_number * 0.8):]\n",
    "    patient_id_test.extend(bbox_patient_index_list)\n",
    "    patient_id_test = list(set(patient_id_test))\n",
    "\n",
    "\n",
    "    print(\"train:{} valid:{} test:{}\".format(len(patient_id_train), len(patient_id_valid), len(patient_id_test)))\n",
    "\n",
    "    # test_set = test_set+left_out_patient_id\n",
    "    # print(\"train:{} valid:{} test:{}\".format(len(train_set), len(valid_set), len(test_set)))\n",
    "    \n",
    "    pathologies_name_list = prj_consts.DISEASE_list\n",
    "    NIH_patients_and_labels_file = 'Data_Entry_2017.csv'\n",
    "    \n",
    "    labels_df = pd.read_csv(os.path.join(other_data_dir, NIH_patients_and_labels_file))\n",
    "    \n",
    "    # # create and save train/test/validation partitions list\n",
    "    \n",
    "    def process_images(current_df, patient_ids):\n",
    "        image_name_index = []\n",
    "        image_labels = {}\n",
    "        for individual_patient in tqdm.tqdm(patient_ids):\n",
    "            for _, row in current_df[current_df['Patient ID'] == individual_patient].iterrows():\n",
    "                processed_image_name = row['Image Index']\n",
    "                if processed_image_name in ignored_images_set:\n",
    "                    pass\n",
    "                else:\n",
    "                    image_name_index.append(processed_image_name)\n",
    "                    image_labels[processed_image_name] = np.zeros(14, dtype=np.uint8)\n",
    "                    for disease_index, ele in enumerate(pathologies_name_list):\n",
    "                        if re.search(ele, row['Finding Labels'], re.IGNORECASE):\n",
    "                            image_labels[processed_image_name][disease_index] = 1\n",
    "                        else:\n",
    "                            # redundant code but just to make it more readable\n",
    "                            image_labels[processed_image_name][disease_index] = 0\n",
    "                    # print(\"processed\", row['Image Index'])\n",
    "        return image_name_index, image_labels\n",
    "\n",
    "    train_data_index, train_labels = process_images(labels_df, patient_id_train)\n",
    "    valid_data_index, valid_labels = process_images(labels_df, patient_id_valid)\n",
    "    test_data_index, test_labels = process_images(labels_df, patient_id_test)\n",
    "\n",
    "    print(\"train, valid, test image number is:\", len(train_data_index), len(valid_data_index), len(test_data_index))\n",
    "\n",
    "    # save the data\n",
    "    labels_all = {}\n",
    "    labels_all.update(train_labels)\n",
    "    labels_all.update(valid_labels)\n",
    "    labels_all.update(test_labels)\n",
    "\n",
    "    partition_dict = {'train': train_data_index, 'test': test_data_index, 'valid': valid_data_index}\n",
    "\n",
    "    with open(os.path.join(data_partitions_dir,'labels14_unormalized_cleaned.pickle'), 'wb') as f:\n",
    "        pickle.dump(labels_all, f)\n",
    "\n",
    "    with open(os.path.join(data_partitions_dir,'partition14_unormalized_cleaned.pickle'), 'wb') as f:\n",
    "        pickle.dump(partition_dict, f)\n",
    "\n",
    "    # also save the patient id partitions for pytorch training    \n",
    "    with open(os.path.join(data_partitions_dir,'train_test_valid_data_partitions.pickle'), 'wb') as f:\n",
    "        pickle.dump([patient_id_train,patient_id_valid,\n",
    "                     patient_id_test,\n",
    "                    list(set(bbox_patient_index_list))], f)    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a training script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the training script to a file to run on the compute source later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {os.path.join(script_folder, train_script_filename)}\n",
    "\n",
    "import sys, os\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data-folder', type=str, dest='data_folder', help='data folder')\n",
    "args = parser.parse_args()\n",
    "print(args)\n",
    "print('Data folder is at:', args.data_folder)\n",
    "base_dir = args.data_folder\n",
    "print('List all files: ', os.listdir(args.data_folder))\n",
    "\n",
    "import src.azure_chestxray_utils as azure_chestxray_utils\n",
    "import src.azure_chestxray_keras_utils as azure_chestxray_keras_utils\n",
    "\n",
    "import subprocess\n",
    "\n",
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "ia.seed(1)\n",
    "\n",
    "import cv2\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ReduceLROnPlateau, Callback, ModelCheckpoint\n",
    "import numpy as np\n",
    "import pickle\n",
    "from keras_contrib.applications.densenet import DenseNetImageNet121\n",
    "from keras.layers import Dense\n",
    "from keras.models import Model\n",
    "from keras.utils import multi_gpu_model\n",
    "from tensorflow.python.client import device_lib\n",
    "import warnings\n",
    "from keras.utils import Sequence\n",
    "import tensorflow as tf\n",
    "import get_data\n",
    "\n",
    "\n",
    "def get_available_gpus():\n",
    "    \"\"\"\n",
    "\n",
    "    Returns: list of GPUs available in the system\n",
    "\n",
    "    \"\"\"\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "#     try:\n",
    "#         out_str = subprocess.run([\"nvidia-smi\", \"--query-gpu=gpu_name\", \"--format=csv\"], stdout=subprocess.PIPE).stdout\n",
    "#         out_list = out_str.decode(\"utf-8\").split('\\n')\n",
    "#         out_list = out_list[1:-1]\n",
    "#         return out_list\n",
    "#     except Exception as e:\n",
    "#         print(e)\n",
    "\n",
    "\n",
    "# multi GPU model checkpoint. copied from https://github.com/keras-team/keras/issues/8463\n",
    "class MultiGPUCheckpointCallback(Callback):\n",
    "\n",
    "    def __init__(self, filepath, base_model, monitor='val_loss', verbose=0,\n",
    "                 save_best_only=False, save_weights_only=False,\n",
    "                 mode='auto', period=1):\n",
    "        super(MultiGPUCheckpointCallback, self).__init__()\n",
    "        self.base_model = base_model\n",
    "        self.monitor = monitor\n",
    "        self.verbose = verbose\n",
    "        self.filepath = filepath\n",
    "        self.save_best_only = save_best_only\n",
    "        self.save_weights_only = save_weights_only\n",
    "        self.period = period\n",
    "        self.epochs_since_last_save = 0\n",
    "\n",
    "        if mode not in ['auto', 'min', 'max']:\n",
    "            warnings.warn('ModelCheckpoint mode %s is unknown, '\n",
    "                          'fallback to auto mode.' % (mode),\n",
    "                          RuntimeWarning)\n",
    "            mode = 'auto'\n",
    "\n",
    "        if mode == 'min':\n",
    "            self.monitor_op = np.less\n",
    "            self.best = np.Inf\n",
    "        elif mode == 'max':\n",
    "            self.monitor_op = np.greater\n",
    "            self.best = -np.Inf\n",
    "        else:\n",
    "            if 'acc' in self.monitor or self.monitor.startswith('fmeasure'):\n",
    "                self.monitor_op = np.greater\n",
    "                self.best = -np.Inf\n",
    "            else:\n",
    "                self.monitor_op = np.less\n",
    "                self.best = np.Inf\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        self.epochs_since_last_save += 1\n",
    "        if self.epochs_since_last_save >= self.period:\n",
    "            self.epochs_since_last_save = 0\n",
    "            filepath = self.filepath.format(epoch=epoch + 1, **logs)\n",
    "            if self.save_best_only:\n",
    "                current = logs.get(self.monitor)\n",
    "                if current is None:\n",
    "                    warnings.warn('Can save best model only with %s available, '\n",
    "                                  'skipping.' % (self.monitor), RuntimeWarning)\n",
    "                else:\n",
    "                    if self.monitor_op(current, self.best):\n",
    "                        if self.verbose > 0:\n",
    "                            print('Epoch %05d: %s improved from %0.5f to %0.5f,'\n",
    "                                  ' saving model to %s'\n",
    "                                  % (epoch + 1, self.monitor, self.best,\n",
    "                                     current, filepath))\n",
    "                        self.best = current\n",
    "                        if self.save_weights_only:\n",
    "                            self.base_model.save_weights(filepath, overwrite=True)\n",
    "                        else:\n",
    "                            self.base_model.save(filepath, overwrite=True)\n",
    "                    else:\n",
    "                        if self.verbose > 0:\n",
    "                            print('Epoch %05d: %s did not improve' %\n",
    "                                  (epoch + 1, self.monitor))\n",
    "            else:\n",
    "                if self.verbose > 0:\n",
    "                    print('Epoch %05d: saving model to %s' % (epoch + 1, filepath))\n",
    "                if self.save_weights_only:\n",
    "                    self.base_model.save_weights(filepath, overwrite=True)\n",
    "                else:\n",
    "                    self.base_model.save(filepath, overwrite=True)\n",
    "\n",
    "# generator for train and validation data\n",
    "# use the Sequence class per issue https://github.com/keras-team/keras/issues/1638\n",
    "class DataGenSequence(Sequence):\n",
    "    def __init__(self, labels, image_file_index, current_state, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "        print(\"BATCH SIZE 2\", self.batch_size)\n",
    "        self.labels = labels\n",
    "        self.img_file_index = image_file_index\n",
    "        self.current_state = current_state\n",
    "        self.len = len(self.img_file_index) // self.batch_size\n",
    "        print(\"for DataGenSequence\", current_state, \"total rows are:\", len(self.img_file_index), \", len is\", self.len)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # print(\"loading data segmentation\", idx)\n",
    "        # make sure each batch size has the same amount of data\n",
    "        current_batch = self.img_file_index[idx * self.batch_size: (idx + 1) * self.batch_size]\n",
    "        X = np.empty((self.batch_size, resized_height, resized_width, num_channel))\n",
    "        y = np.empty((self.batch_size, num_classes))\n",
    "\n",
    "        for i, image_name in enumerate(current_batch):\n",
    "            path = os.path.join(nih_chest_xray_data_dir, image_name)\n",
    "            # loading data\n",
    "\n",
    "            img = cv2.resize(cv2.imread(path), (resized_height, resized_width)).astype(np.float32)\n",
    "            X[i, :, :, :] = img\n",
    "            y[i, :] = labels[image_name]\n",
    "\n",
    "            # only do random flipping in training status\n",
    "        if self.current_state == 'train':\n",
    "            x_augmented = seq.augment_images(X)\n",
    "        else:\n",
    "            x_augmented = X\n",
    "\n",
    "        return x_augmented, y\n",
    "\n",
    "\n",
    "# loss function\n",
    "def unweighted_binary_crossentropy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        y_true: true labels\n",
    "        y_pred: predicted labels\n",
    "\n",
    "    Returns: the sum of binary cross entropy loss across all the classes\n",
    "\n",
    "    \"\"\"\n",
    "    return K.sum(K.binary_crossentropy(y_true, y_pred))\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    \"\"\"\n",
    "\n",
    "    Returns: a model with specified weights\n",
    "\n",
    "    \"\"\"\n",
    "    # define the model, use pre-trained weights for image_net\n",
    "    base_model = DenseNetImageNet121(input_shape=(224, 224, 3),\n",
    "                                     weights='imagenet',\n",
    "                                     include_top=False,\n",
    "                                     pooling='avg')\n",
    "\n",
    "    x = base_model.output\n",
    "    predictions = Dense(14, activation='sigmoid')(x)\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "\n",
    "\n",
    "    #call get_data script\n",
    "#     base_dir = get_data.download_data()\n",
    "    print(\"BASE DIR:\", base_dir)\n",
    "    get_data.preprocess_data(base_dir)\n",
    "    \n",
    "    # create the file path variables \n",
    "    # paths are typically container level dirs mapped to a host dir for data persistence.\n",
    "    prj_consts = azure_chestxray_utils.chestxray_consts()\n",
    "\n",
    "    data_base_input_dir=os.path.join(base_dir, os.path.join(*(prj_consts.BASE_INPUT_DIR_list)))\n",
    "    data_base_output_dir=os.path.join(base_dir, os.path.join(*(prj_consts.BASE_OUTPUT_DIR_list))) \n",
    "\n",
    "    # data used for training\n",
    "    nih_chest_xray_data_dir=os.path.join(data_base_input_dir, \n",
    "                                        os.path.join(*(prj_consts.ChestXray_IMAGES_DIR_list)))\n",
    "\n",
    "    data_partitions_dir=os.path.join(data_base_output_dir, os.path.join(*(prj_consts.DATA_PARTITIONS_DIR_list))) \n",
    "\n",
    "    partition_path = os.path.join(data_partitions_dir, 'partition14_unormalized_cleaned.pickle')\n",
    "    label_path = os.path.join(data_partitions_dir,'labels14_unormalized_cleaned.pickle')\n",
    "\n",
    "    # global variables\n",
    "\n",
    "    weights_dir = os.path.join(data_base_output_dir, os.path.join(*(prj_consts.MODEL_WEIGHTS_DIR_list))) \n",
    "    os.makedirs(weights_dir, exist_ok = True)\n",
    "\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"  # see issue #152\n",
    "    # get number of available GPUs\n",
    "    num_gpu = len(get_available_gpus())\n",
    "#     num_gpu = 1\n",
    "    # keras multi_gpu_model slices the data to different GPUs. see https://keras.io/utils/#multi_gpu_model for more details.\n",
    "    batch_size =num_gpu *  48 # 64 seems t obe too much on NC12 \n",
    "    print(\"NUM GPU:\", num_gpu)\n",
    "\n",
    "    # make force_restart = False if you continue a previous train session, make it True to start from scratch\n",
    "    force_restart = False\n",
    "\n",
    "    initial_lr = 0.001\n",
    "    resized_height = 224\n",
    "    resized_width = 224\n",
    "    # resized_height = prj_consts.CHESTXRAY_MODEL_EXPECTED_IMAGE_HEIGHT\n",
    "    # resized_width = prj_consts.CHESTXRAY_MODEL_EXPECTED_IMAGE_WIDTH\n",
    "    num_channel = 3\n",
    "    num_classes = 14\n",
    "    epochs = 250\n",
    "\n",
    "    seq = iaa.Sequential([\n",
    "    iaa.Fliplr(0.5),  # horizontal flips\n",
    "    iaa.Affine(rotate=(-15, 15)),  # random rotate image\n",
    "    iaa.Affine(scale=(0.8, 1.1)),  # randomly scale the image\n",
    "    ], random_order=True)  # apply augmenters in random order\n",
    "\n",
    "    if num_gpu > 1:\n",
    "        print(\"using\", num_gpu, \"GPUs\")\n",
    "        # build model\n",
    "        with tf.device('/cpu:0'):\n",
    "            model_single_gpu = build_model()\n",
    "        # model_single_gpu.load_weights(weights_path)\n",
    "\n",
    "        # convert to multi-gpu model\n",
    "        model_multi_gpu = multi_gpu_model(model_single_gpu, gpus=num_gpu)\n",
    "        model_checkpoint = MultiGPUCheckpointCallback(\n",
    "            os.path.join(weights_dir, 'azure_chest_xray_14_weights_712split_epoch_{epoch:03d}_val_loss_{val_loss:.4f}.hdf5'),\n",
    "            model_single_gpu, monitor='val_loss', save_weights_only=True)\n",
    "\n",
    "    else:\n",
    "        print(\"using single GPU\")\n",
    "        model_multi_gpu = build_model()\n",
    "        model_checkpoint = ModelCheckpoint(\n",
    "            os.path.join(weights_dir, 'azure_chest_xray_14_weights_712split_epoch_{epoch:03d}_val_loss_{val_loss:.4f}.hdf5'),\n",
    "            monitor='val_loss', save_weights_only=False)\n",
    "\n",
    "\n",
    "    num_workers = num_gpu*2 #*10\n",
    "\n",
    "    model_multi_gpu.compile(optimizer=Adam(lr=initial_lr), loss=unweighted_binary_crossentropy)\n",
    "\n",
    "    reduce_lr_on_plateau = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, min_lr=1e-6)\n",
    "\n",
    "    callbacks = [model_checkpoint, reduce_lr_on_plateau]\n",
    "\n",
    "    with open(label_path, 'rb') as f:\n",
    "        labels = pickle.load(f)\n",
    "\n",
    "    with open(partition_path, 'rb') as f:\n",
    "        partition = pickle.load(f)\n",
    "    \n",
    "    print(\"BATCH SIZE 1\", batch_size)\n",
    "    model_multi_gpu.fit_generator(generator=DataGenSequence(labels, partition['train'], current_state='train', batch_size = batch_size),\n",
    "                                epochs=epochs,\n",
    "                                verbose=1,\n",
    "                                callbacks=callbacks,\n",
    "                                workers=num_workers,\n",
    "                                # max_queue_size=32,\n",
    "                                # shuffle=False,\n",
    "                                validation_data=DataGenSequence(labels, partition['valid'], current_state='validation', batch_size = batch_size)\n",
    "                                # validation_steps=1\n",
    "                                )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure and Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the run configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
    "\n",
    "run_config_2 = RunConfiguration(framework = \"python\")\n",
    "run_config_2.target = gpu_cluster_name\n",
    "run_config_2.environment.docker.enabled = True\n",
    "\n",
    "run_config_2.environment.docker.base_image = 'kateyuan/chestxraynoaml:1.0.4' #DEFAULT_CPU_IMAGE #'nvidia/cuda:9.0-cudnn7-devel' \n",
    "print('Base Docker image is:', run_config_2.environment.docker.base_image )\n",
    "\n",
    "run_config_2.environment.python.user_managed_dependencies = True\n",
    "run_config_2.environment.python.interpreter_path = '/opt/conda/bin/python'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Estimator object and mount datastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.estimator import Estimator\n",
    "from azureml.core import Run\n",
    "\n",
    "script_params = {\n",
    "    '--data-folder': ds.as_mount(),\n",
    "}\n",
    "\n",
    "\n",
    "est = Estimator(source_directory=script_folder,\n",
    "                script_params=script_params,\n",
    "                compute_target=gpu_cluster,\n",
    "                entry_script='train.py',\n",
    "                environment_definition=run_config_2.environment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = exp.submit(config=est)\n",
    "run.wait_for_completion(show_output = True)\n",
    "run.get_portal_url()\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download models (optional)\n",
    "# ds.download(target_path='./models',\n",
    "#             prefix='data/chestxray/output/weights_tmpdir',\n",
    "#             show_progress=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:aml-sdk-conda-env] *",
   "language": "python",
   "name": "conda-env-aml-sdk-conda-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
