{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access credentials file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_path_to_sys_path(path_to_append):\n",
    "    if not (any(path_to_append in paths for paths in sys.path)):\n",
    "        sys.path.append(path_to_append)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials_path = './../not_shared'\n",
    "credentials_file_name = os.path.join(credentials_path, 'get_data_access_secrets.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_path_to_sys_path(credentials_path)         \n",
    "import get_data_access_secrets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up workspace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This notebook was created using version 1.0.8 of the Azure ML SDK\n",
      "You are currently using version 1.0.8 of the Azure ML SDK\n"
     ]
    }
   ],
   "source": [
    "#Create config to workspace\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core import Experiment, Run, Workspace\n",
    "\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"This notebook was created using version 1.0.8 of the Azure ML SDK\")\n",
    "print(\"You are currently using version\", azureml.core.VERSION, \"of the Azure ML SDK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscription_id, resource_group, workspace_name, workspace_region = get_data_access_secrets.get_subscription_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '/subscriptions/edf507a2-6235-46c5-b560-fd463ba2e771/resourceGroups/ghiordanchestxray03rsg/providers/Microsoft.MachineLearningServices/workspaces/sdk-chest-xray',\n",
       " 'name': 'sdk-chest-xray',\n",
       " 'location': 'eastus',\n",
       " 'type': 'Microsoft.MachineLearningServices/workspaces',\n",
       " 'workspaceid': 'fa6444a9-7aaa-47e7-be2f-4f59b6368ef7',\n",
       " 'description': '',\n",
       " 'friendlyName': 'sdk-chest-xray',\n",
       " 'creationTime': '2019-01-25T17:08:07.1455099+00:00',\n",
       " 'containerRegistry': '/subscriptions/edf507a2-6235-46c5-b560-fd463ba2e771/resourcegroups/ghiordanchestxray03rsg/providers/microsoft.containerregistry/registries/sdkchestacrtufoedhv',\n",
       " 'keyVault': '/subscriptions/edf507a2-6235-46c5-b560-fd463ba2e771/resourcegroups/ghiordanchestxray03rsg/providers/microsoft.keyvault/vaults/sdkchestkeyvaultvychezka',\n",
       " 'applicationInsights': '/subscriptions/edf507a2-6235-46c5-b560-fd463ba2e771/resourcegroups/ghiordanchestxray03rsg/providers/microsoft.insights/components/sdkchestinsightsqegtsmwh',\n",
       " 'identityPrincipalId': 'b0a12d93-0702-492b-950e-fa49175ec047',\n",
       " 'identityTenantId': '72f988bf-86f1-41af-91ab-2d7cd011db47',\n",
       " 'identityType': 'SystemAssigned',\n",
       " 'storageAccount': '/subscriptions/edf507a2-6235-46c5-b560-fd463ba2e771/resourcegroups/ghiordanchestxray03rsg/providers/microsoft.storage/storageaccounts/sdkcheststorageqnwmfkeu'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#new workspace\n",
    "ws = Workspace.create(name = workspace_name,\n",
    "                      subscription_id = subscription_id,\n",
    "                      resource_group = resource_group, \n",
    "                      location = workspace_region,\n",
    "                      create_resource_group = True,\n",
    "                      exist_ok = True)\n",
    "ws.get_details()\n",
    "\n",
    "# ws = Workspace.from_config()\n",
    "# print('Workspace name: ' + ws.name, \n",
    "#       'Azure region: ' + ws.location, \n",
    "#       'Subscription id: ' + ws.subscription_id, \n",
    "#       'Resource group: ' + ws.resource_group, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote the config file config.json to: /datadrive01/amlSDKAzureChestXray/code/aml_config/config.json\n"
     ]
    }
   ],
   "source": [
    "ws.write_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment(Name: chestxray-evaluate,\n",
      "Workspace: sdk-chest-xray)\n"
     ]
    }
   ],
   "source": [
    "experiment_name = \"chestxray-evaluate\"\n",
    "\n",
    "from azureml.core import Experiment\n",
    "exp = Experiment(workspace=ws, name=experiment_name)\n",
    "\n",
    "print(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore_name, container_name, account_name, account_key = get_data_access_secrets.get_blob_credentials()\n",
    "# ds = Datastore.register_azure_blob_container(workspace=ws, datastore_name = datastore_name, container_name = container_name, account_name = account_name, account_key = account_key)\n",
    "ds = Datastore.get(workspace=ws, datastore_name=datastore_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chestxraydatastore AzureBlob chestxraystorage xraycontainer\n"
     ]
    }
   ],
   "source": [
    "print(ds.name, ds.datastore_type, ds.account_name, ds.container_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Allocate Compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing gpu cluster\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# Choose a name for your GPU cluster\n",
    "gpu_cluster_name = \"gpucluster\"\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    gpu_cluster = ComputeTarget(workspace=ws, name=gpu_cluster_name)\n",
    "    print(\"Found existing gpu cluster\")\n",
    "except ComputeTargetException:\n",
    "    print(\"Creating new gpucluster\")\n",
    "    \n",
    "    # Specify the configuration for the new cluster\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size=\"Standard_NC24s_v3\",\n",
    "                                                           min_nodes=0,\n",
    "                                                           max_nodes=1)\n",
    "    # Create the cluster with the specified name and configuration\n",
    "    gpu_cluster = ComputeTarget.create(ws, gpu_cluster_name, compute_config)\n",
    "\n",
    "    # Wait for the cluster to complete, show the output log\n",
    "    gpu_cluster.wait_for_completion(show_output=True)\n",
    "    \n",
    "     # For a more detailed view of current AmlCompute status, use the 'status' property    \n",
    "#     print(compute_target.status.serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Scoring Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_folder = './xray_scripts'\n",
    "eval_script_filename = 'eval.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./xray_scripts/eval.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {os.path.join(script_folder, eval_script_filename)}\n",
    "\n",
    "import sys, os\n",
    "\n",
    "import src.azure_chestxray_utils as azure_chestxray_utils\n",
    "import src.azure_chestxray_keras_utils as azure_chestxray_keras_utils\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data-folder', type=str, dest='data_folder', help='data folder')\n",
    "args = parser.parse_args()\n",
    "print(args)\n",
    "print('Data folder is at:', args.data_folder)\n",
    "base_dir = args.data_folder\n",
    "\n",
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "ia.seed(1)\n",
    "\n",
    "import cv2\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ReduceLROnPlateau, Callback, ModelCheckpoint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from keras_contrib.applications.densenet import DenseNetImageNet121\n",
    "from keras.layers import Dense\n",
    "from keras.models import Model\n",
    "from keras.utils import multi_gpu_model\n",
    "import keras_contrib\n",
    "from tensorflow.python.client import device_lib\n",
    "import warnings\n",
    "from keras.utils import Sequence\n",
    "import tensorflow as tf\n",
    "from sklearn import metrics\n",
    "\n",
    "def get_available_gpus():\n",
    "    \"\"\"\n",
    "    Returns: number of GPUs available in the system\n",
    "    \"\"\"\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "\n",
    "# generator for train and validation data\n",
    "# use the Sequence class per issue https://github.com/keras-team/keras/issues/1638\n",
    "class DataGenSequence(Sequence):\n",
    "    def __init__(self, labels, image_file_index, current_state, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.img_file_index = image_file_index\n",
    "        self.current_state = current_state\n",
    "        self.len = len(self.img_file_index) // self.batch_size\n",
    "        print(\"for DataGenSequence\", current_state, \"total rows are:\", len(self.img_file_index), \", len is\", self.len)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        print(\"loading data segmentation\", idx)\n",
    "        # make sure each batch size has the same amount of data\n",
    "        current_batch = self.img_file_index[idx * self.batch_size: (idx + 1) * self.batch_size]\n",
    "        X = np.empty((self.batch_size, resized_height, resized_width, num_channel))\n",
    "        y = np.empty((self.batch_size, num_classes))\n",
    "\n",
    "        for i, image_name in enumerate(current_batch):\n",
    "            path = os.path.join(nih_chest_xray_data_dir, image_name)\n",
    "\n",
    "            # loading data\n",
    "            img = cv2.resize(cv2.imread(path), (resized_height, resized_width)).astype(np.float32)\n",
    "            X[i, :, :, :] = img\n",
    "            y[i, :] = labels[image_name]\n",
    "           \n",
    "            # only do random flipping in training status\n",
    "        if self.current_state == 'train':\n",
    "            # this is different from the training code\n",
    "            x_augmented = X\n",
    "        else:\n",
    "            x_augmented = X\n",
    "        \n",
    "        return x_augmented, y\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    prj_consts = azure_chestxray_utils.chestxray_consts()\n",
    "\n",
    "    #Organize directories\n",
    "    data_base_input_dir=os.path.join(base_dir, \n",
    "                                     os.path.join(*(prj_consts.BASE_INPUT_DIR_list)))\n",
    "    data_base_output_dir=os.path.join(base_dir, \n",
    "                                      os.path.join(*(prj_consts.BASE_OUTPUT_DIR_list)))\n",
    "\n",
    "    weights_dir = os.path.join(data_base_output_dir, os.path.join(*(prj_consts.MODEL_WEIGHTS_DIR_list))) \n",
    "    fully_trained_weights_dir = os.path.join(data_base_output_dir, os.path.join(*(prj_consts.FULLY_PRETRAINED_MODEL_DIR_list))) \n",
    "\n",
    "    nih_chest_xray_data_dir=os.path.join(data_base_input_dir, \n",
    "                                         os.path.join(*(prj_consts.ChestXray_IMAGES_DIR_list)))\n",
    "\n",
    "    data_partitions_dir=os.path.join(data_base_output_dir, \n",
    "                                    os.path.join(*(prj_consts.DATA_PARTITIONS_DIR_list)))  \n",
    "    label_path = os.path.join(data_partitions_dir,'labels14_unormalized_cleaned.pickle')\n",
    "    partition_path = os.path.join(data_partitions_dir, 'partition14_unormalized_cleaned.pickle')\n",
    "\n",
    "#     models_file_name= [os.path.join(weights_dir, \n",
    "#                                     'azure_chest_xray_14_weights_712split_epoch_300_val_loss_361.7687.hdf5')] \n",
    "    models_file_name= [os.path.join(fully_trained_weights_dir, \n",
    "                                   'azure_chest_xray_14_weights_712split_epoch_250_val_loss_179-4776.hdf5')] #EDIT THIS ACCORDINGLY\n",
    "\n",
    "    num_gpu = get_available_gpus()\n",
    "    # get number of available GPUs\n",
    "    print(\"num of GPUs:\", len(get_available_gpus()))\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "    \n",
    "    resized_height = 224\n",
    "    resized_width = 224\n",
    "    num_channel = 3\n",
    "    num_classes = 14\n",
    "    batch_size = 512 \n",
    "\n",
    "    pathologies_name_list = prj_consts.DISEASE_list\n",
    "    pathologies_name_list\n",
    "\n",
    "    stanford_result = [0.8094, 0.9248, 0.8638, 0.7345, 0.8676, 0.7802, 0.7680, 0.8887, 0.7901, 0.8878, 0.9371, 0.8047,\n",
    "                       0.8062, 0.9164]\n",
    "\n",
    "\n",
    "    with open(label_path, 'rb') as f:\n",
    "        labels = pickle.load(f)\n",
    "\n",
    "    with open(partition_path, 'rb') as f:\n",
    "        partition = pickle.load(f)\n",
    "\n",
    "    # load test data\n",
    "    X_test = np.empty((len(partition['test']), 224, 224, 3), dtype=np.float32)\n",
    "    y_test = np.empty((len(partition['test']) - len(partition['test']) % batch_size, 14), dtype=np.float32)\n",
    "\n",
    "    for i, npy in enumerate(partition['test']):\n",
    "        if (i < len(y_test)):\n",
    "            # round to batch_size\n",
    "            y_test[i, :] = labels[npy]\n",
    "\n",
    "    print(\"len of result is\", len(y_test))\n",
    "    y_pred_list = np.empty((len(models_file_name), len(partition['test']), 14), dtype=np.float32)\n",
    "\n",
    "    # individual models\n",
    "    for index, current_model_file in enumerate(models_file_name):\n",
    "        print(current_model_file)\n",
    "    #     model = load_model(current_model_file)\n",
    "        model = azure_chestxray_keras_utils.build_model(keras_contrib.applications.densenet.DenseNetImageNet121); model.load_weights(current_model_file)\n",
    "\n",
    "        print('evaluation for model', current_model_file)\n",
    "        # y_pred = model.predict(X_test)\n",
    "\n",
    "        y_pred = model.predict_generator(generator=DataGenSequence(labels, partition['test'], current_state='test', batch_size = batch_size),\n",
    "                                         workers=32, verbose=1, max_queue_size=1)\n",
    "        print(\"result shape\", y_pred.shape)\n",
    "\n",
    "        # add one fake row of ones in both test and pred values to avoid:\n",
    "        # ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.\n",
    "        y_test = np.insert(y_test, 0, np.ones((y_test.shape[1],)), 0)\n",
    "        y_pred = np.insert(y_pred, 0, np.ones((y_pred.shape[1],)), 0)\n",
    "\n",
    "        df = pd.DataFrame(columns=['Disease', 'Our AUC Score', 'Stanford AUC Score'])\n",
    "        scores_list = []\n",
    "        for d in range(14):\n",
    "            auc = metrics.roc_auc_score(y_test[:, d], y_pred[:, d])\n",
    "            df.loc[d] = [pathologies_name_list[d],\n",
    "                         auc,\n",
    "                         stanford_result[d]]\n",
    "            scores_list.append(auc)\n",
    "        \n",
    "        df['Delta'] = df['Stanford AUC Score'] - df['Our AUC Score']\n",
    "        df.to_csv(current_model_file + \".csv\", index=False)\n",
    "        print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.runconfig import RunConfiguration, DataReferenceConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
    "\n",
    "\n",
    "run_config = RunConfiguration(framework = \"python\")\n",
    "run_config.target = gpu_cluster_name\n",
    "run_config.environment.docker.enabled = True\n",
    "\n",
    "run_config.environment.docker.base_image = 'kateyuan/chestxraynoaml:1.0.4' #DEFAULT_CPU_IMAGE #'nvidia/cuda:9.0-cudnn7-devel' \n",
    "\n",
    "run_config.environment.python.user_managed_dependencies = True\n",
    "run_config.environment.python.interpreter_path = '/opt/conda/bin/python'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.estimator import Estimator\n",
    "\n",
    "script_params = {\n",
    "    '--data-folder': ds.as_mount(),\n",
    "}\n",
    "\n",
    "\n",
    "est = Estimator(source_directory=script_folder,\n",
    "                script_params=script_params,\n",
    "                compute_target=gpu_cluster,\n",
    "                entry_script='eval.py',\n",
    "                environment_definition=run_config.environment)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: chestxray-evaluate_1548950647840\n",
      "\n",
      "Streaming azureml-logs/60_control_log.txt\n",
      "=========================================\n",
      "\n",
      "Streaming log file azureml-logs/60_control_log.txt\n",
      "Warning: Couldn't instantiate AppInsights telemetry client. Telemetry disabled.\n",
      "Streaming log file azureml-logs/80_driver_log.txt\n",
      "\n",
      "Streaming azureml-logs/80_driver_log.txt\n",
      "========================================\n",
      "\n",
      "Warning: Couldn't instantiate AppInsights telemetry client. Telemetry disabled.\n",
      "Warning: Unable to import azureml.history. Output collection disabled.\n",
      "Using TensorFlow backend.\n",
      "Namespace(data_folder='/mnt/batch/tasks/shared/LS_root/jobs/sdk-chest-xray/azureml/chestxray-evaluate_1548950647840/mounts/chestxraydatastore')\n",
      "Data folder is at: /mnt/batch/tasks/shared/LS_root/jobs/sdk-chest-xray/azureml/chestxray-evaluate_1548950647840/mounts/chestxraydatastore\n",
      "2019-01-31 16:08:37.222315: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2019-01-31 16:08:38.039587: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: \n",
      "name: Tesla V100-PCIE-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.38\n",
      "pciBusID: 6e93:00:00.0\n",
      "totalMemory: 15.78GiB freeMemory: 15.37GiB\n",
      "2019-01-31 16:08:38.276414: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 1 with properties: \n",
      "name: Tesla V100-PCIE-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.38\n",
      "pciBusID: 8340:00:00.0\n",
      "totalMemory: 15.78GiB freeMemory: 15.37GiB\n",
      "2019-01-31 16:08:38.595205: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 2 with properties: \n",
      "name: Tesla V100-PCIE-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.38\n",
      "pciBusID: 9c9c:00:00.0\n",
      "totalMemory: 15.78GiB freeMemory: 15.37GiB\n",
      "2019-01-31 16:08:38.864131: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 3 with properties: \n",
      "name: Tesla V100-PCIE-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.38\n",
      "pciBusID: b154:00:00.0\n",
      "totalMemory: 15.78GiB freeMemory: 15.37GiB\n",
      "2019-01-31 16:08:38.864252: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0, 1, 2, 3\n",
      "2019-01-31 16:08:40.376790: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2019-01-31 16:08:40.376861: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 1 2 3 \n",
      "2019-01-31 16:08:40.376872: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N N N N \n",
      "2019-01-31 16:08:40.376878: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 1:   N N N N \n",
      "2019-01-31 16:08:40.376884: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 2:   N N N N \n",
      "2019-01-31 16:08:40.376889: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 3:   N N N N \n",
      "2019-01-31 16:08:40.377915: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/device:GPU:0 with 14873 MB memory) -> physical GPU (device: 0, name: Tesla V100-PCIE-16GB, pci bus id: 6e93:00:00.0, compute capability: 7.0)\n",
      "2019-01-31 16:08:40.378487: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/device:GPU:1 with 14873 MB memory) -> physical GPU (device: 1, name: Tesla V100-PCIE-16GB, pci bus id: 8340:00:00.0, compute capability: 7.0)\n",
      "2019-01-31 16:08:40.379270: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/device:GPU:2 with 14873 MB memory) -> physical GPU (device: 2, name: Tesla V100-PCIE-16GB, pci bus id: 9c9c:00:00.0, compute capability: 7.0)\n",
      "2019-01-31 16:08:40.379976: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/device:GPU:3 with 14873 MB memory) -> physical GPU (device: 3, name: Tesla V100-PCIE-16GB, pci bus id: b154:00:00.0, compute capability: 7.0)\n",
      "2019-01-31 16:08:40.381869: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0, 1, 2, 3\n",
      "2019-01-31 16:08:40.381983: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2019-01-31 16:08:40.381997: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 1 2 3 \n",
      "2019-01-31 16:08:40.382004: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N N N N \n",
      "2019-01-31 16:08:40.382015: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 1:   N N N N \n",
      "2019-01-31 16:08:40.382021: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 2:   N N N N \n",
      "2019-01-31 16:08:40.382037: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 3:   N N N N \n",
      "2019-01-31 16:08:40.382930: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/device:GPU:0 with 14873 MB memory) -> physical GPU (device: 0, name: Tesla V100-PCIE-16GB, pci bus id: 6e93:00:00.0, compute capability: 7.0)\n",
      "2019-01-31 16:08:40.383037: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/device:GPU:1 with 14873 MB memory) -> physical GPU (device: 1, name: Tesla V100-PCIE-16GB, pci bus id: 8340:00:00.0, compute capability: 7.0)\n",
      "2019-01-31 16:08:40.383172: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/device:GPU:2 with 14873 MB memory) -> physical GPU (device: 2, name: Tesla V100-PCIE-16GB, pci bus id: 9c9c:00:00.0, compute capability: 7.0)\n",
      "2019-01-31 16:08:40.383285: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/device:GPU:3 with 14873 MB memory) -> physical GPU (device: 3, name: Tesla V100-PCIE-16GB, pci bus id: b154:00:00.0, compute capability: 7.0)\n",
      "num of GPUs: 4\n",
      "len of result is 32768\n",
      "/mnt/batch/tasks/shared/LS_root/jobs/sdk-chest-xray/azureml/chestxray-evaluate_1548950647840/mounts/chestxraydatastore/data/chestxray/output/fully_trained_models/azure_chest_xray_14_weights_712split_epoch_250_val_loss_179-4776.hdf5\n",
      "2019-01-31 16:08:41.137335: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0, 1, 2, 3\n",
      "2019-01-31 16:08:41.137497: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2019-01-31 16:08:41.137510: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 1 2 3 \n",
      "2019-01-31 16:08:41.137518: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N N N N \n",
      "2019-01-31 16:08:41.137524: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 1:   N N N N \n",
      "2019-01-31 16:08:41.137530: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 2:   N N N N \n",
      "2019-01-31 16:08:41.137536: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 3:   N N N N \n",
      "2019-01-31 16:08:41.138518: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14873 MB memory) -> physical GPU (device: 0, name: Tesla V100-PCIE-16GB, pci bus id: 6e93:00:00.0, compute capability: 7.0)\n",
      "2019-01-31 16:08:41.138695: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 14873 MB memory) -> physical GPU (device: 1, name: Tesla V100-PCIE-16GB, pci bus id: 8340:00:00.0, compute capability: 7.0)\n",
      "2019-01-31 16:08:41.139276: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 14873 MB memory) -> physical GPU (device: 2, name: Tesla V100-PCIE-16GB, pci bus id: 9c9c:00:00.0, compute capability: 7.0)\n",
      "2019-01-31 16:08:41.139845: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 14873 MB memory) -> physical GPU (device: 3, name: Tesla V100-PCIE-16GB, pci bus id: b154:00:00.0, compute capability: 7.0)\n",
      "Downloading data from https://github.com/titu1994/DenseNet/releases/download/v3.0/DenseNet-BC-121-32-no-top.h5\n",
      "\n",
      "    8192/33199896 [..............................] - ETA: 0s\n",
      " 3137536/33199896 [=>............................] - ETA: 0s\n",
      "12001280/33199896 [=========>....................] - ETA: 0s\n",
      "19439616/33199896 [================>.............] - ETA: 0s\n",
      "29048832/33199896 [=========================>....] - ETA: 0s\n",
      "33202176/33199896 [==============================] - 0s 0us/step\n",
      "Weights for the model were loaded successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation for model /mnt/batch/tasks/shared/LS_root/jobs/sdk-chest-xray/azureml/chestxray-evaluate_1548950647840/mounts/chestxraydatastore/data/chestxray/output/fully_trained_models/azure_chest_xray_14_weights_712split_epoch_250_val_loss_179-4776.hdf5\n",
      "for DataGenSequence test total rows are: 32893 , len is 64\n",
      "loading data segmentation 0\n",
      "loading data segmentation 1\n",
      "loading data segmentation 2\n",
      "\n",
      " 1/64 [..............................] - ETA: 25:50loading data segmentation 3\n",
      "\n",
      " 2/64 [..............................] - ETA: 13:08loading data segmentation 4\n",
      "\n",
      " 3/64 [>.............................] - ETA: 8:54 loading data segmentation 5\n",
      "\n",
      " 4/64 [>.............................] - ETA: 9:21loading data segmentation 6\n",
      "\n",
      " 5/64 [=>............................] - ETA: 7:30loading data segmentation 7\n",
      "\n",
      " 6/64 [=>............................] - ETA: 6:18loading data segmentation 8\n",
      "\n",
      " 7/64 [==>...........................] - ETA: 6:47loading data segmentation 9\n",
      "\n",
      " 8/64 [==>...........................] - ETA: 5:55loading data segmentation 10\n",
      "\n",
      " 9/64 [===>..........................] - ETA: 5:16loading data segmentation 11\n",
      "\n",
      "10/64 [===>..........................] - ETA: 5:37loading data segmentation 12\n",
      "\n",
      "11/64 [====>.........................] - ETA: 5:05loading data segmentation 13\n",
      "\n",
      "12/64 [====>.........................] - ETA: 4:39loading data segmentation 14\n",
      "\n",
      "13/64 [=====>........................] - ETA: 4:55loading data segmentation 15\n",
      "\n",
      "14/64 [=====>........................] - ETA: 4:31loading data segmentation 16\n",
      "\n",
      "15/64 [======>.......................] - ETA: 4:12loading data segmentation 17\n",
      "\n",
      "16/64 [======>.......................] - ETA: 4:24loading data segmentation 18\n",
      "\n",
      "17/64 [======>.......................] - ETA: 4:05loading data segmentation 19\n",
      "\n",
      "18/64 [=======>......................] - ETA: 3:49loading data segmentation 20\n",
      "\n",
      "19/64 [=======>......................] - ETA: 3:58loading data segmentation 21\n",
      "\n",
      "20/64 [========>.....................] - ETA: 3:43loading data segmentation 22\n",
      "\n",
      "21/64 [========>.....................] - ETA: 3:30loading data segmentation 23\n",
      "\n",
      "22/64 [=========>....................] - ETA: 3:35loading data segmentation 24\n",
      "\n",
      "23/64 [=========>....................] - ETA: 3:23loading data segmentation 25\n",
      "\n",
      "24/64 [==========>...................] - ETA: 3:11loading data segmentation 26\n",
      "\n",
      "25/64 [==========>...................] - ETA: 3:16loading data segmentation 27\n",
      "\n",
      "26/64 [===========>..................] - ETA: 3:05loading data segmentation 28\n",
      "\n",
      "27/64 [===========>..................] - ETA: 2:55loading data segmentation 29\n",
      "\n",
      "28/64 [============>.................] - ETA: 2:58loading data segmentation 30\n",
      "\n",
      "29/64 [============>.................] - ETA: 2:48loading data segmentation 31\n",
      "\n",
      "30/64 [=============>................] - ETA: 2:39loading data segmentation 32\n",
      "\n",
      "31/64 [=============>................] - ETA: 2:40loading data segmentation 33\n",
      "\n",
      "32/64 [==============>...............] - ETA: 2:32loading data segmentation 34\n",
      "\n",
      "33/64 [==============>...............] - ETA: 2:23loading data segmentation 35\n",
      "\n",
      "34/64 [==============>...............] - ETA: 2:24loading data segmentation 36\n",
      "\n",
      "35/64 [===============>..............] - ETA: 2:16loading data segmentation 37\n",
      "\n",
      "36/64 [===============>..............] - ETA: 2:09loading data segmentation 38\n",
      "\n",
      "37/64 [================>.............] - ETA: 2:09loading data segmentation 39\n",
      "\n",
      "38/64 [================>.............] - ETA: 2:01loading data segmentation 40\n",
      "\n",
      "39/64 [=================>............] - ETA: 1:54loading data segmentation 41\n",
      "\n",
      "40/64 [=================>............] - ETA: 1:53loading data segmentation 42\n",
      "\n",
      "41/64 [==================>...........] - ETA: 1:47loading data segmentation 43\n",
      "\n",
      "42/64 [==================>...........] - ETA: 1:40loading data segmentation 44\n",
      "\n",
      "43/64 [===================>..........] - ETA: 1:39loading data segmentation 45\n",
      "\n",
      "44/64 [===================>..........] - ETA: 1:32loading data segmentation 46\n",
      "\n",
      "45/64 [====================>.........] - ETA: 1:26loading data segmentation 47\n",
      "\n",
      "46/64 [====================>.........] - ETA: 1:24loading data segmentation 48\n",
      "\n",
      "47/64 [=====================>........] - ETA: 1:18loading data segmentation 49\n",
      "\n",
      "48/64 [=====================>........] - ETA: 1:12loading data segmentation 50\n",
      "\n",
      "49/64 [=====================>........] - ETA: 1:10loading data segmentation 51\n",
      "\n",
      "50/64 [======================>.......] - ETA: 1:04loading data segmentation 52\n",
      "\n",
      "51/64 [======================>.......] - ETA: 58s loading data segmentation 53\n",
      "\n",
      "52/64 [=======================>......] - ETA: 55sloading data segmentation 54\n",
      "\n",
      "53/64 [=======================>......] - ETA: 50sloading data segmentation 55\n",
      "\n",
      "54/64 [========================>.....] - ETA: 45sloading data segmentation 56\n",
      "\n",
      "55/64 [========================>.....] - ETA: 41sloading data segmentation 57\n",
      "\n",
      "56/64 [=========================>....] - ETA: 36sloading data segmentation 58\n",
      "\n",
      "57/64 [=========================>....] - ETA: 31sloading data segmentation 59\n",
      "\n",
      "58/64 [==========================>...] - ETA: 27sloading data segmentation 60\n",
      "\n",
      "59/64 [==========================>...] - ETA: 22sloading data segmentation 61\n",
      "\n",
      "60/64 [===========================>..] - ETA: 18sloading data segmentation 62\n",
      "\n",
      "61/64 [===========================>..] - ETA: 13sloading data segmentation 63\n",
      "\n",
      "62/64 [============================>.] - ETA: 9s \n",
      "63/64 [============================>.] - ETA: 4s\n",
      "loading data segmentation 1\n",
      "\n",
      "64/64 [==============================] - 294s 5s/step\n",
      "result shape (32768, 14)\n",
      "samples [[1.30584955e-01 1.03530481e-04 1.65203977e-02 1.18964113e-01\n",
      "  3.62093635e-02 6.71616495e-02 7.08097033e-03 1.68859325e-02\n",
      "  1.08667826e-02 1.77453709e-04 2.34915949e-02 3.49088199e-02\n",
      "  1.68607888e-11 5.03099849e-03]\n",
      " [7.43334293e-02 8.80900014e-04 3.03676352e-02 8.47575366e-02\n",
      "  6.21812381e-02 6.36033863e-02 6.24456117e-03 1.68349035e-02\n",
      "  1.01473816e-02 1.40508520e-04 3.83650996e-02 4.93366271e-02\n",
      "  9.72876554e-12 6.67845551e-03]\n",
      " [1.96586743e-01 7.74184184e-04 2.56899744e-02 8.95130187e-02\n",
      "  3.07457745e-02 6.73885718e-02 8.46532825e-03 7.84604251e-03\n",
      "  1.08039044e-02 1.97616042e-04 1.86838377e-02 2.71407180e-02\n",
      "  4.59153271e-11 1.31555228e-02]\n",
      " [8.93161371e-02 2.69314274e-04 1.72542464e-02 1.27382591e-01\n",
      "  3.93785723e-02 9.23436284e-02 1.00648375e-02 2.26219296e-02\n",
      "  1.39066428e-02 8.12675222e-04 1.87695008e-02 3.57498638e-02\n",
      "  1.04052857e-10 2.45645037e-03]\n",
      " [2.92468667e-01 3.42679070e-03 5.40183671e-02 8.51931870e-02\n",
      "  3.18594091e-02 4.13537510e-02 1.12706479e-02 7.48301763e-03\n",
      "  1.25518451e-02 2.49696808e-04 1.36623345e-02 3.37724611e-02\n",
      "  9.44197429e-11 2.46215295e-02]]\n",
      "samples real [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "average AUC score: 0.7908467015237187\n",
      "               Disease  Our AUC Score  Stanford AUC Score     Delta\n",
      "0          Atelectasis       0.755515              0.8094  0.053885\n",
      "1         Cardiomegaly       0.885565              0.9248  0.039235\n",
      "2             Effusion       0.844962              0.8638  0.018838\n",
      "3         Infiltration       0.693922              0.7345  0.040578\n",
      "4                 Mass       0.782377              0.8676  0.085223\n",
      "5               Nodule       0.679992              0.7802  0.100208\n",
      "6            Pneumonia       0.691666              0.7680  0.076334\n",
      "7         Pneumothorax       0.819752              0.8887  0.068948\n",
      "8        Consolidation       0.762951              0.7901  0.027149\n",
      "9                Edema       0.852193              0.8878  0.035607\n",
      "10           Emphysema       0.776393              0.9371  0.160707\n",
      "11            Fibrosis       0.741811              0.8047  0.062889\n",
      "12  Pleural Thickening       1.000000              0.8062 -0.193800\n",
      "13              Hernia       0.784754              0.9164  0.131646\n",
      "\n",
      "\n",
      "The experiment completed successfully. Finalizing run...\n",
      "Logging experiment finalizing status in history service\n",
      "Warning: Couldn't instantiate AppInsights telemetry client. Telemetry disabled.\n",
      "terminate called recursively\n",
      "terminate called without an active exception\n",
      "\n",
      "Execution Summary\n",
      "=================\n",
      "RunId: chestxray-evaluate_1548950647840\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.35 s, sys: 461 ms, total: 8.81 s\n",
      "Wall time: 9min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from azureml.core import Run\n",
    "run = exp.submit(est)\n",
    "run.wait_for_completion(show_output = True)\n",
    "run.get_portal_url()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
